{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbconvert": {
     "hide_code": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ODE\n",
    "\n",
    "$$\n",
    "  y''' + x y'' + 3 y' + y = e^{−x}\n",
    "$$\n",
    "\n",
    "into a first order system of ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "Step by step we introduce\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  u &= y' \\\\\n",
    "  v &= u' \\\\\n",
    "    &= y''.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can therefore write the ODE into a system of ODEs. The first order ODEs for $y$ and $u$ are given by the definitions above. The ODE for $v$ is given from the original equation, substituting in the definition of $u$ where appropriate, to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\begin{pmatrix} y \\\\ u \\\\ v \\end{pmatrix}' & = \\begin{pmatrix} u \\\\ v \\\\ e^{-x} - x y'' - 3 y' - y \\end{pmatrix} \\\\\n",
    "  & = \\begin{pmatrix} u \\\\ v \\\\ e^{-x} - x v - 3 u - y \\end{pmatrix}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show by Taylor expansion that the backwards differencing estimate of $f(x)$,\n",
    "\n",
    "$$\n",
    "  f(x) = \\frac{f(x) − f(x − h)}{h}\n",
    "$$\n",
    "\n",
    "is first order accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "We have the Taylor series expansion of $f(x − h)$ about $x$ is\n",
    "\n",
    "$$\n",
    "  f(x − h) = f(x) − h f'(x) + \\frac{h^2}{2!} f''(x) + {\\mathcal O} (h^3).\n",
    "$$\n",
    "\n",
    "Substituting this in to the backwards difference formula we find\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{f(x) - f(x - h)}{h} & = \\frac{f(x) - f(x) + h f'(x) + \\frac{h^2}{2!} f''(x) + {\\mathcal O} (h^3)}{h} \\\\\n",
    "  & = f'(x) + {\\mathcal O} (h)\n",
    "\\end{align}\n",
    "\n",
    "Therefore the difference between the exact derivative $f'$ and the backwards difference estimate is $\\propto h$ and hence the finite difference estimate is first order accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Taylor expansion to derive a symmetric or central difference estimate of $f^{(4)}(x)$ on a grid with spacing $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "For this we need the Taylor expansions\n",
    "\n",
    "\\begin{align*}\n",
    "    f(x + h) & = f(x) + h f^{(1)}(x) + \\frac{h^2}{2!} f^{(2)}(x) +\n",
    "    \\frac{h^3}{3!} f^{(3)}(x) + \\frac{h^4}{4!} f^{(4)}(x) +\n",
    "    \\frac{h^5}{5!} f^{(5)}(x) + \\dots \\\\\n",
    "    f(x - h) & = f(x) - h f^{(1)}(x) + \\frac{h^2}{2!} f^{(2)}(x) -\n",
    "    \\frac{h^3}{3!} f^{(3)}(x) + \\frac{h^4}{4!} f^{(4)}(x) -\n",
    "    \\frac{h^5}{5!} f^{(5)}(x) + \\dots \\\\\n",
    "    f(x + 2 h) & = f(x) + 2 h f^{(1)}(x) + \\frac{4 h^2}{2!} f^{(2)}(x) +\n",
    "    \\frac{8 h^3}{3!} f^{(3)}(x) + \\frac{16 h^4}{4!} f^{(4)}(x) +\n",
    "    \\frac{32 h^5}{5!} f^{(5)}(x) + \\dots \\\\\n",
    "    f(x - 2 h) & = f(x) - 2 h f^{(1)}(x) + \\frac{4 h^2}{2!} f^{(2)}(x) -\n",
    "    \\frac{8 h^3}{3!} f^{(3)}(x) + \\frac{16 h^4}{4!} f^{(4)}(x) -\n",
    "    \\frac{32 h^5}{5!} f^{(5)}(x) + \\dots \n",
    "\\end{align*}\n",
    "\n",
    "By a central or symmetric difference estimate we mean that the coefficient of $f(x \\pm n h)$ should have the same magnitude. By comparison with central difference estimates for first and second derivatives we see that for odd order derivatives the coefficients should have opposite signs and for even order the same sign.\n",
    "\n",
    "So we write our estimate as\n",
    "\n",
    "\\begin{equation*}\n",
    "    f^{(4)}(x) \\simeq A f(x) + B \\left( f(x + h) + f(x - h) \\right)\n",
    "    + C \\left( f(x + 2 h) +  f(x - 2 h) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "and we then need to constrain the coefficients $A, B, C$. By looking at terms proportional to $h^s$ we see\n",
    "\n",
    "\\begin{align*}\n",
    "    h^0: && 0 & = A + 2 B + 2 C \\\\\n",
    "    h^1: && 0 & = 0 \\\\\n",
    "    h^2: && 0 & = B + 4 C \\\\\n",
    "    h^3: && 0 & = 0 \\\\\n",
    "    h^4: && \\frac{1}{h^4} & = \\frac{B}{12} + \\frac{16 C}{12}. \n",
    "\\end{align*}\n",
    "\n",
    "This gives three constraints on our three unknowns so we cannot go to higher order. Solving the equations gives\n",
    "\n",
    "\\begin{equation*}\n",
    "    A = \\frac{6}{h^4}, \\qquad B = -\\frac{4}{h^4}, \\qquad C =\n",
    "    \\frac{1}{h^4}.\n",
    "\\end{equation*}\n",
    "\n",
    "Writing it out in obvious notation we have\n",
    "\n",
    "\\begin{equation*}\n",
    "    f_1^{(4)} = \\frac{1}{h^4} \\left( 6 f_i - 4 (f_{i+1} + f_{i-1}) +\n",
    "      (f_{i+2} + f_{i-2}) \\right).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the convergence rate of Euler's method and the Euler predictor-corrector method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "Euler's method converges as $h$ and the predictor-corrector method as $h^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain when multistage methods such as Runge-Kutta methods are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "Multistage methods require only one vector of initial data, which must be provided to completely specify the IVP; that is, the method is self-starting. It is also easy to adapt a multistage method to use variable step sizes; that is, to make the algorithm adaptive depending on local error estimates in order to keep the global error within some tolerance. Finally, it is relatively easy to theoretically show convergence. Combining this we see that multistage methods are useful as generic workhorse algorithms and in cases where the function defining the IVP may vary widely in behaviour, so that adaptive algorithms are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the power method for finding the largest eigenvalue of a matrix. In particular, explain why it is simpler to find the absolute value, and how to find the phase information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "The idea behind the power method is that most easily seen by writing out a generic vector ${\\bf x}$ in terms of the eigenvectors of the matrix $A$ whose eigenvalues we wish to find,\n",
    "\n",
    "$$\n",
    "  {\\bf x} = \\sum_{i=1}^N a_i {\\bf e}_i,\n",
    "$$\n",
    "\n",
    "where we assume that the eigenvectors are ordered such that the associated eigenvalues have the order $|\\lambda_1 | > |\\lambda_2 | \\ge |\\lambda_3 | \\ge \\dots \\ge |\\lambda_N |$. Note that we always assume that there is a unique eigenvalue $\\lambda_1$ with largest magnitude.\n",
    "\n",
    "We then note that multiplying this generic vector by the matrix $A$ a number of times gives\n",
    "\n",
    "$$\n",
    "  A^k {\\bf x} = \\lambda_1^k \\sum_{i=1}^N a_i \\left( \\frac{\\lambda_i}{\\lambda_1} \\right)^k {\\bf e}_i.\n",
    "$$\n",
    "\n",
    "We then note that, for $i = 1$, the ratio of the eigenvalues $(\\lambda_i / \\lambda_1)^k$ must tend to zero as $k \\to \\infty$. Therefore in the limit we will \"pick out\" $\\lambda_1$.\n",
    "\n",
    "Of course, to actually get the eigenvalue itself we have to essentially divide two vectors. That is, we define a sequence $x^{(k)}$ where the initial value $x^{(0)}$ is arbitrary and at each step we multiply by $A$, so that\n",
    "\n",
    "$$\n",
    "  x^{(k)} = A^k x^{(0)}.\n",
    "$$\n",
    "\n",
    "It follows that we can straightforwardly get $\\lambda_1$ by looking at \"the ratio of successive iterations\". E.g.,\n",
    "\n",
    "$$\n",
    "  \\lim_{k \\to \\infty} \\frac{ \\| {\\bf x}_{k+1} \\| }{ \\| {\\bf x}_k \\| } = | \\lambda_1 |.\n",
    "$$\n",
    "\n",
    "This only gives information about the magnitude as we have used the simplest way of getting from a vector to a real number, the absolute value. To retain information about the phase we need to replace the absolute value of the vectors with some linear functional such as the sum of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Euler's method to the ODE\n",
    "\n",
    "$$\n",
    "  y' + 2y = 2 − e^{−4 x}, \\qquad y(0) = 1.\n",
    "$$\n",
    "\n",
    "Find the value of $y(1)$ (analytic answer is $1 − (e^{−2} − e^{−4})/2$) and see how your method converges with resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Coding Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from scipy import linalg\n",
    "\n",
    "def euler(f, y0, interval, N = 100):\n",
    "    \"\"\"\n",
    "    Solve the IVP y' = f(x, y) on the given interval using N+1 points,\n",
    "    (counting the initial point) with initial data y0.\n",
    "    \"\"\"\n",
    "    \n",
    "    h = (interval[1] - interval[0]) / N\n",
    "    x = numpy.linspace(interval[0], interval[1], N+1)\n",
    "    y = numpy.zeros((len(y0), N+1))\n",
    "    y[:, 0] = y0\n",
    "    \n",
    "    for i in range(N):\n",
    "        y[:, i+1] = y[:, i] + h * f(x[i], y[:, i])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def fn_q1(x, y):\n",
    "    \"\"\"\n",
    "    Function defining the IVP in question 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 2.0 - numpy.exp(-4.0*x) - 2.0*y\n",
    "\n",
    "# Now do the test\n",
    "\n",
    "exact_y_end = 1.0 - (numpy.exp(-2.0) - numpy.exp(-4.0)) / 2.0\n",
    "\n",
    "# Test at default resolution\n",
    "x, y = euler(fn_q1, numpy.array([1.0]), [0.0, 1.0])\n",
    "print(\"Error at the end point is \", y[:, -1] - exact_y_end)\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.plot(x, y[0, :], 'b-+')\n",
    "pyplot.xlabel('$x$', size = 16)\n",
    "pyplot.ylabel('$y$', size = 16)\n",
    "\n",
    "# Now do the convergence test\n",
    "\n",
    "levels = numpy.array(range(4, 10))\n",
    "Npoints = 2**levels\n",
    "abs_err = numpy.zeros(len(Npoints))\n",
    "for i in range(len(Npoints)):\n",
    "    x, y = euler(fn_q1, numpy.array([1.0]), [0.0, 1.0], Npoints[i])\n",
    "    abs_err[i] = abs(y[0, -1] - exact_y_end)\n",
    "\n",
    "# Best fit to the errors\n",
    "h = 1.0 / Npoints\n",
    "p = numpy.polyfit(numpy.log(h), numpy.log(abs_err), 1)\n",
    "    \n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.loglog(h, abs_err, 'kx')\n",
    "pyplot.loglog(h, numpy.exp(p[1]) * h**(p[0]), 'b-')\n",
    "pyplot.xlabel('$h$', size = 16)\n",
    "pyplot.ylabel('$|$Error$|$', size = 16)\n",
    "pyplot.legend(('Euler Errors', \"Best fit line slope {:.3}\".format(p[0])),\n",
    "              loc = \"upper left\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the standard RK4 method to the above system, again checking that it converges with resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Coding Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "outputs": [],
   "source": [
    "def RK4(f, y0, interval, N = 100):\n",
    "    \"\"\"\n",
    "    Solve the IVP y' = f(x, y) on the given interval using N+1 points\n",
    "    (counting the initial point) with initial data y0.\n",
    "    \"\"\"\n",
    "    \n",
    "    h = (interval[1] - interval[0]) / N\n",
    "    x = numpy.linspace(interval[0], interval[1], N+1)\n",
    "    y = numpy.zeros((len(y0), N+1))\n",
    "    y[:, 0] = y0\n",
    "    \n",
    "    for i in range(N):\n",
    "        k1 = h * f(x[i]          , y[:, i])\n",
    "        k2 = h * f(x[i] + h / 2.0, y[:, i] + k1 / 2.0)\n",
    "        k3 = h * f(x[i] + h / 2.0, y[:, i] + k2 / 2.0)\n",
    "        k4 = h * f(x[i] + h      , y[:, i] + k3)\n",
    "        y[:, i+1] = y[:, i] + (k1 + k4 + 2.0 * (k2 + k3)) / 6.0\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def fn_q2(x, y):\n",
    "    \"\"\"\n",
    "    Function defining the IVP in question 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 2.0 - numpy.exp(-4.0*x) - 2.0*y\n",
    "\n",
    "# Now do the test\n",
    "\n",
    "exact_y_end = 1.0 - (numpy.exp(-2.0) - numpy.exp(-4.0)) / 2.0\n",
    "\n",
    "# Test at default resolution\n",
    "x, y = RK4(fn_q1, numpy.array([1.0]), [0.0, 1.0])\n",
    "print(\"Error at the end point is \", y[:, -1] - exact_y_end)\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.plot(x, y[0, :], 'b-+')\n",
    "pyplot.xlabel('$x$', size = 16)\n",
    "pyplot.ylabel('$y$', size = 16)\n",
    "\n",
    "# Now do the convergence test\n",
    "\n",
    "levels = numpy.array(range(4, 10))\n",
    "Npoints = 2**levels\n",
    "abs_err = numpy.zeros(len(Npoints))\n",
    "for i in range(len(Npoints)):\n",
    "    x, y = RK4(fn_q1, numpy.array([1.0]), [0.0, 1.0], Npoints[i])\n",
    "    abs_err[i] = abs(y[0, -1] - exact_y_end)\n",
    "\n",
    "# Best fit to the errors\n",
    "h = 1.0 / Npoints\n",
    "p = numpy.polyfit(numpy.log(h), numpy.log(abs_err), 1)\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.loglog(h, abs_err, 'kx')\n",
    "pyplot.loglog(h, numpy.exp(p[1]) * h**(p[0]), 'b-')\n",
    "pyplot.xlabel('$h$', size = 16)\n",
    "pyplot.ylabel('$|$Error$|$', size = 16)\n",
    "pyplot.legend(('RK4 Errors', \"Best fit line slope {0:.3}\".format(p[0])),\n",
    "              loc = \"upper left\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code using the power method and inverse power method to compute the largest and smallest eigenvalues of an arbitrary matrix. Apply it to a random $n = 3$ matrix, checking that the correct answer is found. How does the number of iterations required for convergence to a given level vary with the size of the matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Coding Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "outputs": [],
   "source": [
    "def PowerMethod(A, tolerance = 1e-10, MaxSteps = 100):\n",
    "    \"\"\"\n",
    "    Apply the power method to the matrix A to find the\n",
    "    largest eigenvalue in magnitude.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = numpy.size(A, 0)\n",
    "    # Simple initial value\n",
    "    x = numpy.ones(n)\n",
    "    x /= linalg.norm(x)\n",
    "    ratio = 1.0\n",
    "    for k in range(MaxSteps):\n",
    "        ratio_old = ratio\n",
    "        x_old = x.copy()\n",
    "        x = numpy.dot(A, x)\n",
    "        ratio = numpy.sum(x) / numpy.sum(x_old)\n",
    "        x /= linalg.norm(x)\n",
    "        if (abs(ratio - ratio_old) < tolerance):\n",
    "            break\n",
    "    \n",
    "    return ratio, k\n",
    "\n",
    "def InversePowerMethod(A, tolerance = 1e-10, MaxSteps = 100):\n",
    "    \"\"\"\n",
    "    Apply the inverse power method to the matrix A to find the\n",
    "    smallest eigenvalue in magnitude.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = numpy.size(A, 0)\n",
    "    # Simple initial value\n",
    "    x = numpy.ones(n)\n",
    "    x /= linalg.norm(x)\n",
    "    ratio = 1.0\n",
    "    for k in range(MaxSteps):\n",
    "        ratio_old = ratio\n",
    "        x_old = x.copy()\n",
    "        x = linalg.solve(A, x)\n",
    "        ratio = numpy.sum(x) / numpy.sum(x_old)\n",
    "        x /= linalg.norm(x)\n",
    "        if (abs(ratio - ratio_old) < tolerance):\n",
    "            break\n",
    "    \n",
    "    return 1.0/ratio, k\n",
    "\n",
    "# Test on a random 3x3 matrix\n",
    "\n",
    "A = numpy.random.rand(3,3)\n",
    "max_lambda, iterations_max = PowerMethod(A)\n",
    "min_lambda, iterations_min = InversePowerMethod(A)\n",
    "eigenvalues, eigenvectors = linalg.eig(A)\n",
    "print(\"Computed maximum and minimum eigenvalues are\",\n",
    "      max_lambda, min_lambda)\n",
    "print(\"True eigenvalues are\", eigenvalues)\n",
    "\n",
    "# Now check how the number of iterations depends on the matrix size.\n",
    "# As we are computing random matrices, do average of 10 attempts\n",
    "MinMatrixSize = 3\n",
    "MaxMatrixSize = 50\n",
    "Attempts = 10\n",
    "iterations = numpy.zeros((MaxMatrixSize - MinMatrixSize + 1, Attempts))\n",
    "for n in range(MinMatrixSize, MaxMatrixSize):\n",
    "    for a in range(Attempts):\n",
    "        A = numpy.random.rand(n, n)\n",
    "        ratio, iterations[n - MinMatrixSize, a] = PowerMethod(A)\n",
    "\n",
    "ii = numpy.mean(iterations, 1)\n",
    "nn = numpy.array(range(MinMatrixSize, MaxMatrixSize))\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.plot(range(MinMatrixSize, MaxMatrixSize+1),\n",
    "            numpy.mean(iterations, 1), 'kx')\n",
    "pyplot.xlabel('Matrix Size')\n",
    "pyplot.ylabel('Mean number of iterations')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "We see that the number of iterations is practically unchanged with the size of the matrix."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
